#07 — Aléatoire

//citation en exergue
“One thing that traditional computer systems aren’t good at is coin flipping,” says Steve Ward, Professor of Computer Science and Engineering at MIT’s Computer Science and Artificial Intelligence Laboratory. (http://engineering.mit.edu/ask/can-computer-generate-truly-random-number).

Nos machines sont déterministes, ce qui signifie que si l’on leur pose une même question, elles nous donneront chaque fois la même réponse. Ce déterminisme est inhérent au fonctionnement algorithmique de ces machines : en tant qu’objet de désambiguïsation, on attend effectivement d’un algorithme qu’il fournisse un résultat constant.
Edgar Morin (//nbp : « introduction à la pensée complexe ») distingue ainsi les systèmes triviaux des systèmes non-triviaux. Selon sa définition, un système non-trivial est un système dont, connaissant ses *inputs*, nous sommes incapables de déterminer ses *outputs*. Par opposition, un système trivial est alors un système dont le comportement peut être prédit, généralement parce qu’il est pré-déterminé par des règles explicites. Suivant ce raisonnement, il est alors possible de catégoriser dans l’absolu l’Homme comme un système non-trivial, tandis qu’une machine algorithmique entrera dans la catégorie des systèmes triviaux.

Durant le développement de sa réflexion autour du *jeu de l’imitation*, Alan Turing considère la trivialité d’une machine algorithmique comme une limitation importante de son potentiel : comment en effet imiter un comportement humain —non-trivial— à l’aide d’une machine déterministe, et par conséquent triviale ? En considérant l’intégration d’une roulette au fonctionnement de ses machines, Turing pose alors les fondements d’une solution : utiliser des objets capables de résultats aléatoires dans une machine algorithmique lui permettrait de simuler un comportement non-trivial.
(//images de machines de Turing avec roulette, si introuvables autres exemples de tels objets aléatoires)
Permettre à l’algorithme de créer de l’aléatoire, ce serait alors lui offrir la possibilité de s’écarter du déterminisme qui le défini. L’algorithme étant fondamentalement une méthode explicite permettant d’arriver à un résultat déterminé, on peut le considérer comme un objet d’intention. Y intégrer la notion d’aléatoire, c’est alors lui permettre de s’écarter du but pour lequel il a été développé, et le laisser produire des résultats qui ne sont pas attendus, et qui s’éloignent de l’intention initiale.

La notion d’aléatoire semble bien incompatible avec celle de l’algorithme. Pire encore : il est fondamentalement *impossible* pour un algorithme de créer un véritable hasard. John von Neumann, 1951: "Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin. ». Parce que le paradigme déterministe de l’algorithme ne laisse pas la place au hasard, on choisit alors de parler de génération *pseudo-aléatoire*.
En programmation informatique par exemple, une méthode possible pour générer un nombre pseudo-aléatoire consiste à récupérer l’heure exacte (en microseconde) de l’ordinateur à l’instant précis de l’exécution de l’instruction « donne-moi un nombre pseudo-aléatoire entre 1 et 10», puis à la manipuler pour arriver au résultat escompté. Parce que cette heure est extrêmement précise et qu’il est très compliqué d’en avoir une visibilité exacte, elle suffit généralement à nous faire croire au hasard. (//exemple visuel)
Pour autant, deux occurrences de ce même programme exécutées dans exactement les mêmes conditions produiront le même nombre pseudo-aléatoire. Si ce genre de méthode répond bien à des besoins basiques, les systèmes nécessitant des générations moins triviales et dans l’absolu moins prédictibles (notamment en cryptologie, pour des besoins de sécurité) emploient aujourd’hui des algorithmes très complexes de simulation de l’aléatoire. Ces générations restent toutefois déterminées.

Plusieurs acteurs proposent aujourd’hui des réponses innovantes à ce problème, et mettent en place ce qu’on appelle des *TRNG* (*true random number generators*, des générateurs de nombres « véritablement » aléatoires), par opposition aux précédents *PRNG* (*pseudo random number generator*). S’il existe différentes typologiques de TRNG, ils ont cependant en commun le fait de ne pas être le produit d’un algorithme.
RANDOM.ORG est l’une des solutions qui fait autorité dans ce domaine. Développé en 1998 par Mads Haahr (//nbp : bio : docteur en science de l’informatique et en statistiques), le site propose une série d’outils permettant de générer de « vrais » nombres aléatoires, basés sur la captation de phénomènes météorologiques complexes (le « bruit atmosphérique »). Parce que ces phénomènes sont d’une immense complexité, ils sont à faible échelle totalement imprédictibles, et permettraient ainsi de générer un « véritable » hasard, ou tout du moins un hasard que nous serions incapable de prédire.
Une autre solution intéressante, notamment en regard des travaux d’Alan Turing sur le jeu de l’imitation, est le site RND.FARM, qui se décrit comme « a stream of human generated randomness ». Le système fonctionne en deux temps : il propose tout d’abord une interface de captation de cet « aléatoire humain », où il est demandé à l’utilisateur de déplacer sa souris et de taper au clavier totalement au hasard. Il offre dans un second temps un outil permettant de générer des nombres basés sur les comportements aléatoire précédemment captés. Dans le cas de systèmes algorithmiques visant à simuler le comportement humain, cet outil offre ainsi la possibilité de manipuler des jeux de données fondés sur une sensibilité humaine captée.

Si la manipulation du hasard dans le champ de l’algorithme a effectivement des applications pratiques (cryptologie, simulation de comportements non-triviaux), elle reste basée sur un double paradoxe : concevoir un algorithme aléatoire, c’est aller à l’encontre de sa définition déterministe ; manipuler le hasard à l’intérieur d’un algorithme, c’est essayer de le dompter, de le faire rentrer dans un espace prédéterminé. Pour autant, ce paradoxe peut amener à un algorithme nouveau, plus complexe encore, qui offrirait la possibilité à son créateur de se libérer de l’intention.

(//en exergue, ces deux citations mises en regard)
> Pour faire un poème dadaïste
> Prenez un journal.
> Prenez des ciseaux.
> Choisissez dans le journal un article ayant la longeur que vous comptez donner à votre poème.
> Découpez l'article.
> Découpez ensuite avec soin chacun des mots qui forment cet article et mettez-les dans un sac.
> Agitez doucement.
> Sortez ensuite chaque coupière l'une après l'autre.
> Copiez consciencieusement dans l'ordre où elles ont quitté le sac.
> Le poème vous ressemblera.
> Et vous voilà un écrivain infiniment original et d'une sensibilité charmante, encore qu'incomprise du vulgaire.
— Tristan Tzara: Part VIII of "Dada manifeste sur l'amour faible et l'amour amer",  La Vie des Lettres, 4 (April 1921), 434-443. Reprinted in: Oeuvres complètes, Vol.1. Paris, 1975, p. 382.

> « The work of art which is the result of random choice does not have any meaning and there is no key to decipher it. Though such a work of art may be esthetically relevant, the spontaneous associations of the observer have no connexion with the author. The observer may attach meaning to the work on the basis of his own imagination, but this meaning would not be inherent in the work of art itself. We could go one step further and assert that there is no link between the author and the work of art if the medium, the method of creating and the content have been chosen at random. » — Vladimir Bonacic: "Umjetnost kao funkcija subjekta, spoznaje i vremena." ("Art as function of subject, cognition and time.") In: Boris Kelemen & Radoslav Putar: Dijalog sa Strojem / Dialog with the Machine. Zagreb: Galerije Grada Zagreba, 1971, p. 140.


// ci-dessous en sidenote ?
Bien sûr, la question de la prédictibilité et du déterminisme faisant débat, nous devons convoquer Laplace : 
> « Nous devons envisager l'état présent de l'univers comme l'effet de son état antérieur, et comme la cause de celui qui va suivre. Une intelligence qui, pour un instant donné, connaîtrait toutes les forces dont la nature est animée et la situation respective des êtres qui la composent, si d'ailleurs elle était assez vaste pour soumettre ces données à l'analyse, embrasserait dans la même formule les mouvements des plus grands corps de l'univers et ceux du plus léger atome : rien ne serait incertain pour elle, et l'avenir, comme le passé, serait présent à ses yeux. L'esprit humain offre, dans la perfection qu'il a su donner à l'astronomie, une faible esquisse de cette intelligence. Ses découvertes en mécanique et en géométrie, jointes à celles de la pesanteur universelle, l'ont mis à portée de comprendre dans les mêmes expressions analytiques les états passés et futurs du système du monde. En appliquant la même méthode à quelques autres objets de ses connaissances, il est parvenu à ramener à des lois générales les phénomènes observés, et à prévoir ceux que les circonstances données doivent faire éclore. » — Pierre-Simon de Laplace, Essai philosophique sur les probabilités  
Si l’on suit une pensée Laplacienne, tout système tombe ainsi dans l’absolu dans la trivialité, pour peu que nous possédions l’intelligence et les connaissances nécessaires à la compréhension des règles qui l’animent. Ce paradigme déterministe invalide alors toute question du hasard, de la chance et de l’aléatoire.



