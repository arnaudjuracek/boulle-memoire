#08 — Erreur

L’algorithme est un objet qui permet d’arriver à un résultat. Ce résultat est soit la réponse à un problème (Euclide : quelle est le plus grand commun diviseur entre ces deux nombres ?), soit le produit d’une série de manipulation (une recette de cuisine par exemple). Fondamentalement, il est donc un objet qui résout.
Si ce résultat n’est pas forcément consistant ou prédictible, on n’en attend pas moins que l’algorithme *fonctionne*, c’est-à-dire qu’il produise le résultat pour lequel il a été conçu. Dans le cas contraire, l’algorithme est inexploitable, *cassé*.
L’erreur n’est donc pas souhaitable dans une pratique algorithmique classique.

Parce que les algorithmes modernes sont de plus en plus complexes et qu’ils impliquent souvent un nombre très importants de sous-algorithmes inter-dépendants, il peut cependant parfois être difficile d’assurer un programme parfaitement fiable. Si l’industrie du logiciel alloue des budgets conséquents au *débuggage* et développe des tests méthodiques de vérifications des algorithmes, il est parfois plus simple —et souvent moins cher— de compter sur un seuil d’erreur admissible. Les algorithmes d’optimisation de trajets (ou *pathfinding*) par exemple choisissent de déplacer le curseur vers une réponse probable en un temps minimum d’exécution, plutôt que  d’arriver à un résultat parfait en un temps souvent bien plus long. 

Il est pourtant communément accepté qu’un algorithme —et par amalgame une machine— ne doive pas, ne puisse pas faire d’erreur. Une recherche récemment menée à l’université de Pennsylvanie (//nbp : http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2466040) démontre à ce propos que les gens sont généralement réticents à donner une seconde chance à un algorithme qui semble s’être trompé, même s’ils acceptent le fait qu’il soit à priori plus fiable qu’un humain.
> « Bien sûr, aucun algorithme n’est parfait, soulignent les chercheurs, mais nous sommes plus enclins à leur reprocher leur imperfections qu’aux humains. En fait, quand une machine fait une erreur, nous avons tendance à penser qu’elle pourra la refaire - ce qui est certainement de moins en moins vrai à l’heure des machines apprenantes. » — http://alireailleurs.tumblr.com/post/111360073624/laversion-aux-algorithmes-knowledge-wharton

Dans son essai *Computing Machinery and Intelligence*, Alan Turing attaque le mythe de la machine parfaite, en distinguant notamment deux types d’erreurs : 
> « The claim that "machines cannot make mistakes" seems a curious one. [...] It seems to me that this criticism depends on a confusion between two kinds of mistake, we may call them "**errors of functioning**" and "**errors of conclusion**".
> [...] In philosophical discussions one likes to ignore the possibility of such errors; one is therefore discussing "abstract machines." These abstract machines are mathematical fictions rather than physical objects. By definition they are incapable of errors of functioning. In this sense we can truly say that "machines can never make mistakes." Errors of conclusion can only arise when some meaning is attached to the output signals from the machine. »
Selon lui, l’erreur algorithmique peut être de deux types : de fonctionnement et de conclusion.

L’erreur de fonctionnement est physique : la machine est littéralement cassée, un élément constitutif ne fonctionne pas, entraînant un comportement global erratique. En 1946, alors que Hopper travaille sur l’ordinateur électro-mécanique Mark II d’Harvard, il découvre qu’une erreur est causée par une mite coincée dans un relai électrique de la machine. Cette erreur causée par un insecte donne naissance au terme *bug*, qui dès lors décrira un problème de fonctionnement dans une machine, et par abus de langage une erreur informatique, quelle que soit sa typologie.
Comme le souligne Turing, l’erreur de fonctionnement ne peut donc exister si l’on s’intéresse uniquement à l’algorithme en tant qu’abstraction mathématique. Dans un monde parfait, l’incarnation matérielle d’un algorithme (que ce soit au travers du métier à tisser Jacquard ou au sein d’un microprocesseur) ne devrait pas entraîner d’erreur de fonctionnement.

Par opposition l’erreur de conclusion est méta-physique, et ne peut être évacuée en abstraisant l’algorithme. Un bon exemple d’une telle erreur peut être trouvé dans le *Cycle des Robots* d’Isaac Asimov. Dans l’une des nouvelles du recueil, Powell et Donovan, deux roboticiens, sont appelés pour réparer un robot *bugué*. Ce robot, chargé de transporter d’un point A à un point B un objet dans l’hostile environnement martien, est occupé depuis plusieurs jours à tourner autour d’un cratère, faillant à sa mission. En étudiant l’algorithme définissant le comportement dudit robot, les deux roboticiens comprennent rapidement que l’erreur est une erreur logique, et non mécanique : si le robot à effectivement pour tâche de se déplacer d’un point A à un point B, la résolution de l’itinéraire qu’il emprunte doit cependant respecter les lois de la robotique, et notamment la consigne de ne pas se mettre en danger. En s’approchant du cratère, le robot c’est ainsi retrouvé face à un problème insoluble : réussir à traverser le cratère pour rejoindre sa destination sans pour autant se mettre en danger. La mise en équilibre des deux instructions « ne te mets pas en danger » et « rejoins ta destination » le pousse en toute logique à se déplacer en cercle autour du cratère. (//un petit schéma sera le bienvenu)
Du point de vue du robot, il n’y a pas d’erreur : l’exécution de l’algorithme est correcte, les consignes données sont suivies à la perfection. « Errors of conclusion can only arise when some meaning is attached to the output signals from the machine », précise Turing : l’erreur n’est erreur que dans le contexte plus général de la tâche attribuée au robot. Du point de vue de l’algorithme, l’erreur de conclusion n’existe pas : il n’y a que la froide exécution qui compte.

Est-il alors possible de concevoir un algorithme capable de donner sens à l’erreur, d’être capable de la prendre en compte ? On considère aujourd’hui cette question comme l’une des principales limites à l’automatisation par l’algorithme, et de nombreuses recherches sont menées en ce sens. Turing encore une fois offre dès 1950 les bases réflexives à cette problématique, en imaginant une machine « apprenante » (on parle de *machine learning*), capable de déceler une erreur de conclusion et d’adapter son fonctionnement en conséquence. 
Le site web Akinator est un exemple très trivial d’une telle machine « apprenante ». Akinator (//nbp : http://fr.akinator.com/), c’est un « génie numérique » capable de deviner en une vingtaine de questions le nom de la personnalité à qui vous pensez. Si son taux de réussite est impressionnant (et trouble beaucoup de techno-novices), il lui arrive de se tromper. Et c’est là toute la subtilité de son fonctionnement : lorsqu’il donne la réponse qu’il pense être la bonne, l’algorithme d’Akinator demande à l’utilisateur de valider cette réponse ; si elle est fausse, l’utilisateur est alors invité à entrer le nom de la personne à qui il pensait véritablement, et Akinator en tiendra compte pour les utilisateurs à venir. Plus Akinator se trompe, moins il a de chance de se tromper.

Il est donc tout à fait envisageable de concevoir un avenir proche où l’algorithme ne serait plus capable d’erreur, devenant ainsi la fiction mathématique décrite par Turing. Pour autant, il convient de questionner l’intérêt d’un monde où l’erreur algorithmique serait bannie : s’il est évidemment rassurant de pouvoir faire confiance à un Algorithme omniprésent et régent, la perfection algorithmique n’irait-elle pas à l’encontre de notre imparfaite Humanité ?

La reine autoproclamée du « glitch » Rosa Menkman (//nbp bio) propose ainsi d’amener à une culture visuelle de l’erreur informatique. En informatique, le *glitch*, sorte de bruit informationnel, pourrait se résumer à un bug qui n’empêche pas pour autant le fonctionnement de la machine (par exemple un problème d’encodage vidéo qui entraîne des sautes d’images, des pixels qui « bavent », etc...). Time Magazine décrit le *glitch* comme « a spaceman's word for irritating disturbances » (//nbp : July 23, 1965, Time Magazine). Menkman pose en 2005 les bases du mouvement Glitch Art au travers d’un manifeste, le « Glitch Momentum » : 
> « The glitch makes the computer itself suddenly appear unconventionally deep, in contrast to the more banal, predictable surface-level behaviours of ‘normal’ machines and systems. In this way, glitches announce a crazy and dangerous kind of moment(um) instantiated and dictated by the machine itself. »
Allant à contre-courant de la tendance à l’élimination méthodique de toute erreur algorithmique, le mouvement Glitch Art se propose alors au contraire de mettre en valeur le Glitch en le rendant sensible, allant jusqu’à le provoquer. En agissant ainsi, les Glitch artistes offrent à voir la complexité algorithmique, en élevant l’erreur au rang d’artefact médiateur du fonctionnement latent de nos machines.
Un glitch, c’est alors le témoin de la complexité subtile des algorithmes.

>  « We call accidents on the computer 'bugs'. Most technologists are trained to seek out and destroy all bugs, but I guess where I differ is that I actually collect and nurture my bugs. They reveal the true nature of the computer. A computer virus is a good example of this phenomenon. Our acknowledging the that computer viruses exist give us a better sense that the computer network today is in many senses a kind of organic entity subject to similar weaknesses as our own body. » — John Maeda, nature + “eye’m hungry” | Fondation Cartier pour l'art contemporain